---
title: "Value of information"
output: rmarkdown::github_document
---

```{r set up,echo=F,message=F}
library( lubridate,quietly = T )
library(grid,quietly = T)
library(DT,quietly = T)
library(scales,quietly = T)
library( ggplot2 ,quietly = T)
library(reshape2,quietly = T)
library(dplyr,quietly = T)
library(RColorBrewer,quietly = T)
library(plotrix,quietly = T)
library(gridExtra)
library(tidyverse)
library(mgcv,quietly = T);
library(knitr,quietly = T);
library(distr,quietly = T);
library(earth,quietly = T)
options(scipen=999)

nSamples <- 5000
fixed_values <- readRDS('fixed_values.Rds')
for(i in 1:length(fixed_values)) assign(names(fixed_values)[i],fixed_values[[i]])
source('pollution_model_functions.R')
```

Value of information (VOI) is a measure of how much one uncertain variable tells you about another uncertain variable. In health-impact modelling, where we have many inputs and an output of interest, we're interested in how much an uncertain input tells us about the uncertain output, so that we can anticipate how useful it would be to us to learn more about that input, particularly if there are many inputs we might consider learning about.

The attached example computes the expected value of perfect partial information (EVPPI). That is, the value of learning a single parameter perfectly. In this example, there are three parameters, and we compare their EVPPI values. This method is similar to sensitivity analyses such as tornado plots, where the influence of individual parameters on the outcome is estimated by considering the ranges in the outcome obtained when parameters are set to their 5th and 95th quantiles in turn.

Beyond EVPPI, other VOI metrics include the expected value of perfect information (EVPI), which is the value of learning all parameters perfectly, and the expected value of sample information (EVSI), which is the value of collecting data that informs knowledge of one parameter or more. Therefore, we have that EVSI &le; EVPPI &le; EVPI. The methods we present here are based on the theory published [here](https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1562932), where the methods are presented formally and the connection to decision-making problems is made explicity in Sections 2.2, 2.3 and 2.4.

# This example

The attached example uses a simplified, idealised health-impact model taken from the "integrated transport and health" suite of models. It consists of a single demographic group, who are female and aged 45 to 50. We have a value for that group's incidence of stroke events, which is a measure of their health burden. Their stroke incidence is `r background_burden`. We're interested to predict the health burden in "scenarios" in which something about their environment changes relative to the "baseline", which is the current state of affairs.

We have an estimate of the background level of PM2.5, a class of pollutants with diameter less than 2.5 micrometers with associations to chronic diseases; we have an estimate of the proportion of PM2.5 that is attributable to car use; we have an estimate of the dose--response relationship between PM2.5 and incidence of stroke; and we have two scenarios, one in which car use decreases, and one in which car use increases. We use a model to predict what the health burden will be in the different scenarios, and we use EVPPI to understand which uncertainties in our model drive the uncertainty in the estimated health burden.




```{r plot parameters,echo=F, fig.height = 5, fig.width = 15}

## INPUTS
parameters <- get_parameters()
#knitr::opts_chunk$set(fig.width=15, fig.height=5) 
par(mar=c(5,5,2,2),mfrow=c(1,3),cex.axis=2,cex.lab=2)
plot(density(parameters[[1]]),frame=F,typ='l',xlab='Background PM2.5',ylab='Density',lwd=2,lty=1,col='navyblue',main='')
plot(density(parameters[[2]]),frame=F,typ='l',xlab='Car fraction',ylab='Density',lwd=2,lty=1,col='navyblue',main='')
x <- 1:60
param_names <- c('alpha','beta','gamma','tmrel')
param_cols <- match(param_names,colnames(strokeDR))
dr_samples <- dose_response(x,alpha=strokeDR[1,param_cols[1]],beta=strokeDR[1,param_cols[2]],gamma=strokeDR[1,param_cols[3]],tmrel=strokeDR[1,param_cols[4]])
plot(x,dr_samples,frame=F,typ='l',xlab='PM2.5',ylab='Relative risk',lwd=2,lty=1,ylim=c(1,2),col='navyblue')#adjustcolor('navyblue', alpha.f = 0.5))
for(i in 2:20){
  dr_samples <- dose_response(x,alpha=strokeDR[i,param_cols[1]],beta=strokeDR[i,param_cols[2]],gamma=strokeDR[i,param_cols[3]],tmrel=strokeDR[i,param_cols[4]])
  lines(x,dr_samples,typ='l',lwd=2,lty=1,col='navyblue')#adjustcolor('navyblue', alpha.f = 0.5))
}
  
```


```{r compute parameters and results,echo=F}

parameter_samples <- matrix(0,nrow=nSamples,ncol=length(parameters))
result <- matrix(0,nrow=nSamples,ncol=length(scenario_travel_ratio))
for(j in 1:nSamples){
  pollution_return <- pollution_calculation(parameters,j)
  parameter_samples[j,] <- pollution_return$parameter_samples
  result[j,] <- pollution_return$scenario_burden
}
colnames(parameter_samples) <- names(parameters)
  
```

# Results

The distributions of expected health burdens in terms of incidence are

```{r plot results,echo=F}
cols <- c('navyblue','hotpink')
par(mar=c(5,5,2,2),cex.lab=1.2,cex.axis=1.2)
plot(density(result[,2]),col=cols[1],frame=F,xlab='Incidence',ylab='Density',main='',lwd=2,xlim=range(result))
lines(density(result[,3]),col=cols[2],lwd=2)
abline(v=background_burden,col='grey',lwd=3,lty=2)
legend(col=cols,legend=c('Scenario 1','Scenario 2'),bty='n',x=min(result),y=2e-3,lwd=2)

```

So what are the parameters that we could most usefully learn to increase precision in our estimates for the two scenarios?

## Tornado plots

A traditional method to answer this question would be to use a tornado plot, where we fix all the parameters except one to the median, evaluate the outcome for the 5th and 95th quantiles of the one parameters, repeat for all parameters, and compare the ranges in outcomes from each parameter range. The quantiles of our parameters are shown below:

```{r quantiles,echo=F}
parameter_names <- c('Background PM2.5','Car fraction','DR function')
quantiles <- t(apply(parameter_samples,2,quantile,c(0.05,0.5,0.95)))[1:2,]
quantiles <- rbind(quantiles,DR_quantile=c(0.05,0.5,0.95))
quantile_print <- quantiles
rownames(quantile_print) <- parameter_names
kable(quantile_print,digits=2, booktabs=TRUE, escape = FALSE)
```

Note that we take the quantiles not for the four dose-response parameters, but rather for the curve they define.


To demonstrate the tornado plot, we consider the case where travel increases:

```{r tornado,echo=F}
tor_result <- matrix(0,ncol=4,nrow=nrow(quantiles))
for(j in 1:nrow(quantiles)){
  params <- as.list(quantiles[,2])
  names(params) <- rownames(quantiles)
  params[[j]] <- quantiles[j,1]
  pollution_return <- pollution_calculation(params,1,full_parameters=parameters)
  tor_result[j,c(1,3)] <- pollution_return$scenario_burden[2:3]# - pollution_return$scenario_burden[1]
  params[[j]] <- quantiles[j,3]
  pollution_return <- pollution_calculation(params,1,full_parameters=parameters)
  tor_result[j,c(2,4)] <- pollution_return$scenario_burden[2:3]# - pollution_return$scenario_burden[1]
}
diff1 <- tor_result[,1]-tor_result[,2]
sorted <- sort(diff1,index.return=T,decreasing=F)



#barplot(sorted$x,horiz=T,names=rownames(quantiles)[sorted$ix],las=2,col='navyblue')
params <- as.list(quantiles[,2])
names(params) <- rownames(quantiles)
pollution_return <- pollution_calculation(params,1,full_parameters=parameters)
base.value <- pollution_return$scenario_burden[3]

df <- as.data.frame(tor_result )
colnames(df)[1:4] <- c('LB','UB','Lower_Bound','Upper_Bound')
df$UL_Difference <- df[,4]-df[,3]
df$Parameter <- parameter_names
order.parameters <- df %>% arrange(UL_Difference) %>%
  mutate(Parameter=factor(x=Parameter, levels=Parameter)) %>%
  select(Parameter) %>% unlist() %>% levels()
width <- 0.95
df.2 <- df %>% 
  gather(key='type', value='output.value', Lower_Bound:Upper_Bound) %>%
  select(Parameter, type, output.value, UL_Difference) %>%
  mutate(Parameter=factor(Parameter, levels=order.parameters),
         ymin=pmin(output.value, base.value),
         ymax=pmax(output.value, base.value),
         xmin=as.numeric(Parameter)-width/2,
         xmax=as.numeric(Parameter)+width/2)

ggplot() + 
  geom_rect(data = df.2, 
            aes(ymax=ymax, ymin=ymin, xmax=xmax, xmin=xmin, fill=type)) +
  theme_bw() + 
  theme(axis.title.y=element_blank(), legend.position = 'bottom',
        legend.title = element_blank()) + 
  geom_hline(yintercept = base.value) +
  scale_x_continuous(breaks = c(1:length(order.parameters)), 
                     labels = order.parameters) +
  coord_flip()

```

However, while the plot is useful for the parameters we might be able to learn on their own, $\mu$ and $\pi$, it is less useful for the parameters for the dose-response curve. This is because it's not the case that we could learn one parameter in particular. In addition, it's also not useful to take the 5th and 95th quantiles of $\beta$, say, while taking the 50th quantiles for the other parameters. This is because they have a joint density.

## EVPPI method

Instead, with EVPPI, we can evaluate the impact of variability in parameters, whilst also considering the distributions of the other parameters. EVPPI is evaluated by regressing the outcome against each parameter in turn, or against a set of parameters. 

```{r univariate EVPPI}
# initialise empty matrix for evppi results
evppi <- matrix(0,ncol=ncol(result)-1,nrow=length(parameter_names))
# loop over results, held in columns, omitting the first (baseline)
for(j in 2:ncol(result)){
  # extract outcome vector y
  y <- result[,j]
  # compute variance
  vary <- var(y)
  # compute for first two (univariate) parameters
  for(i in 1:2){
    # extract parameter vector x
    x <- parameter_samples[,i];
    # write y as a smooth model of x
    model <- gam(y~s(x)); 
    # compute variance in prediction
    pred_var <- mean((y-model$fitted)^2)
    # calculate raw evppi as the expected reduction in variance
    raw_evppi <- vary-pred_var
    # calculate evppi as a percentage of observed variance
    evppi[i,j-1] <- raw_evppi/vary*100;
  }
}

## use earth for four-dimensional parameters
# loop over results, held in columns, omitting the first (baseline)
for(j in 2:ncol(result)){
  # extract outcome vector y
  y <- result[,j]
  # compute variance
  vary <- var(y)
  # write y as a smooth model of xs
  model <- earth(y~parameter_samples[,3:6],degree=4); 
  # compute variance in prediction
  pred_var <- mean((y-model$fitted)^2)
  # calculate raw evppi as the expected reduction in variance
  raw_evppi <- vary-pred_var
  # calculate evppi as a percentage of observed variance
  evppi[3,j-1] <- raw_evppi/vary*100;
}
```

## EVPPI result
```{r EVPPI vector,echo=F}
scenarios <- c('Scenario 1','Scenario 2')
colnames(evppi) <- scenarios
rownames(evppi) <- parameter_names
kable(evppi,digits=1)
```

```{r plot,echo=F,fig.width=10}
#par(mar=c(12.5,5,1,1))
cols <- c('navyblue','hotpink')
names(cols) <- colnames(evppi)
colScale <- scale_fill_manual(values = cols)
meltevppi <- melt(evppi)
colnames(meltevppi) <- c('Parameter','Scenario','EVPPI')
plt <- ggplot() + geom_col(data = meltevppi, aes(x = Parameter,y=EVPPI, fill = Scenario), position = "dodge") + colScale +
  theme(text=element_text(family="Garamond", size=14))
plt + theme(axis.text=element_text(family="Garamond", size=14),axis.title=element_text(family="Garamond", size=14) ,panel.grid.major = element_blank(), panel.grid.minor = element_blank(),  panel.background = element_blank())

#x <- barplot(t(evppi),beside=T,names.arg=parameter_names,las=2,cex.names=1.25,cex.lab=1.25,cex.axis=1.25,col=cols,ylab='EVPPI, % of Var(incidence)')
#legend(legend=scenarios,x=0.5,y=max(evppi)+2,bty='n',fill=cols)
```

So, learning the car fraction of background PM2.5 concentration better would most increase precision for our estimate under a car decrease scenario. Learning the car fraction of background PM2.5 concentration or the dose--response relationship better would most increase precision for our estimate under a car increase scenario.

******

# Model details

The outcome, the incidence, is a number $y$. We considered two scenarios, a decrease in car use (scenario 1) and an increase in car use (scenario 2), but let's consider for now that there is just one, for simplicity of notation, and let's call the change in travel $D$, so that if there were 1,000 km of travel in the baseline, there are 1,000$D$ km of travel in the scenario. So, there is one outcome, $y$, and it is the stroke incidence in the scenario conditions.

There are three uncertain inputs, $\mu$, $\pi$ and $\theta_3$. We define $\mu$, the background PM2.5 concentration, to have a lognormal distribution with mean and variance parameters that we specify. We define $\pi$, the fraction of PM2.5 attributable to cars, to have a Beta distribution with parameters alpha and beta that we specify. 

Then the PM2.5 concentration in the scenario is 

$\text{PM}2.5 = \mu (\pi D + 1 - \pi)$,

that is, the amount contributed by cars, scaled by $D$, added to the amount that exists independently of cars.

The input $\theta_3$ defines the relationship between PM2.5 and stroke. There exists a function, $g_2(\text{PM}2.5,\theta_3)$, that maps the PM2.5 concentation onto the relative risk (RR) of stroke, which is learnt from observational data. The function $g_2(\text{PM}2.5,\theta_3)$ defines a dose--response relationship, where the dose is the PM2.5 and the response is relative risk of stroke. The risk is relative to a PM2.5 value of 0, so the relative risk at PM2.5=0 is 1. We use values from Burnett et al. (2014, doi: 10.1289/ehp.1307049), where $\theta_3=\{\alpha,\beta,\gamma,\tau\}$, and

relative risk of stroke ($R$) = $g_2(\text{PM}2.5,\theta_3) = 1 + \alpha ( 1 - \exp(- \beta ( \text{PM}2.5 - \tau )^{\gamma} ) )$.

The uncertainty about the accuracy of the dose--response relationship is captured through the sampled values of the components of $\theta_3$. 

For our final computation, we also need the relative risk for the baseline, $R_0$:

$R_0 = 1 + (\theta_3-1)g_2(\mu,\theta_3)$.

The scenario RR will be a relative increase or a relative decrease from the baseline RR ($R_0$), and this relationship is applied to the baseline burden of disease in order to estimate the burden of disease in the scenario:

$y=18530{R}/{R_0}$.





